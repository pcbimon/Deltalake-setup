# Use the Spark image with Scala, Java, and Python as the base image
FROM spark:3.5.2-scala2.12-java17-python3-ubuntu
USER root

RUN set -ex; \
    apt-get update; \
    apt-get install -y python3 python3-pip; \
    rm -rf /var/lib/apt/lists/*
# Create the .ivy2 cache directory and set permissions for the Spark user
RUN mkdir -p /home/spark/.ivy2/cache && \
    chown -R spark:spark /home/spark/.ivy2
    
USER spark
# Environment variables for Delta Lake version
ENV DELTA_VERSION=2.4.0
ENV SPARK_HOME=/opt/spark
ENV HADOOP_VERSION=3

# Set working directory to Spark home
WORKDIR ${SPARK_HOME}

# Pre-download Delta Lake dependencies
RUN ${SPARK_HOME}/bin/spark-shell --packages io.delta:delta-core_2.12:${DELTA_VERSION} --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Ensure the Spark home is accessible
ENV PATH="${SPARK_HOME}/bin:${PATH}"

# Expose necessary ports for Spark Master and UI
EXPOSE 7077 8080

# Start Spark Master when the container runs
CMD ["/opt/spark/sbin/start-master.sh", "-p", "7077"]